{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5fdb8cdc",
   "metadata": {},
   "source": [
    "<img src=\"https://hilpisch.com/tpq_logo_bic.png\" width=\"20%\" align=\"right\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d97c1efc",
   "metadata": {},
   "source": [
    "# Python & AI in Asset Management\n",
    "## Covariance Matrix, Eigenvalues & Eigenvectors\n",
    "\n",
    "&copy; Dr. Yves J. Hilpisch<br>\n",
    "AI-Powered by GPT 5.x<br>\n",
    "The Python Quants GmbH | https://tpq.io<br>\n",
    "https://hilpisch.com | https://linktr.ee/dyjh\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65224a1c",
   "metadata": {},
   "source": [
    "## Notebook Goals\n",
    "\n",
    "This notebook is a deep, end-to-end case study of the **2×2 covariance matrix**\n",
    "\n",
    "$$\n",
    "\\Sigma=\\begin{pmatrix}\n",
    "\\sigma_1^2 & \\sigma_{12}\\\\\n",
    "\\sigma_{12} & \\sigma_2^2\n",
    "\\end{pmatrix},\n",
    "$$\n",
    "\n",
    "You will learn to:\n",
    "\n",
    "- move from **state-contingent payoffs** to expectations, variances, covariance, and correlation,\n",
    "- read the **shape of the payoff cloud** in the plane and connect it to the covariance matrix,\n",
    "- interpret **eigenvalues and eigenvectors** as risk magnitudes and principal directions, and\n",
    "- relate these ideas back to **two-asset portfolios**, principal components, and near-singular covariance matrices.\n",
    "\n",
    "The focus is on **geometry and intuition**: the formulas are short, but every key object has a picture and an interpretation.\n",
    "\n",
    "All plots use **Matplotlib** only.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cbcf0a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# plt.style.use('dark_background') # e.g. for notebook only\n",
    "plt.style.use('seaborn-v0_8')\n",
    "\n",
    "np.set_printoptions(precision=6, suppress=True)\n",
    "%config InlineBackend.figure_format = 'svg'\n",
    "\n",
    "def cov_from_states(x, p):\n",
    "    \"\"\"Probability-weighted covariance matrix for discrete states.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    x : array, shape (n_states, n_assets)\n",
    "        State-contingent payoffs/returns (rows=states, cols=assets).\n",
    "    p : array, shape (n_states,)\n",
    "        Probabilities (need not sum to 1; will be normalized).\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    mu : array, shape (n_assets,)\n",
    "        Mean vector E[X]\n",
    "    Sigma : array, shape (n_assets, n_assets)\n",
    "        Covariance matrix E[(X-mu)(X-mu)^T]\n",
    "    \"\"\"\n",
    "    p = np.asarray(p, dtype=float)\n",
    "    p = p / p.sum()\n",
    "    x = np.asarray(x, dtype=float)\n",
    "    mu = (p[:, None] * x).sum(axis=0)\n",
    "    xc = x - mu\n",
    "    Sigma = xc.T @ (p[:, None] * xc)\n",
    "    return mu, Sigma\n",
    "\n",
    "def corr_from_cov(S):\n",
    "    d = np.sqrt(np.diag(S))\n",
    "    return S / np.outer(d, d)\n",
    "\n",
    "def eig_sorted(S):\n",
    "    # For symmetric matrices: eigh returns real eigenvalues + orthonormal eigenvectors.\n",
    "    vals, vecs = np.linalg.eigh(S)\n",
    "    idx = np.argsort(vals)[::-1]\n",
    "    return vals[idx], vecs[:, idx]\n",
    "\n",
    "def unit(v):\n",
    "    v = np.asarray(v, float)\n",
    "    n = np.linalg.norm(v)\n",
    "    return v if n == 0 else v/n\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "def fig_dir():\n",
    "    cwd = Path.cwd()  # current working directory\n",
    "    return cwd / \"figures\"\n",
    "\n",
    "FIG_DIR = fig_dir()\n",
    "FIG_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def save_fig(fig, name):\n",
    "    fig.savefig(\n",
    "        FIG_DIR / name, dpi=300, bbox_inches=\"tight\"\n",
    "    )  # save figure to figures/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfc9c4b4",
   "metadata": {},
   "source": [
    "## 1) Start from primitives: a static economy with discrete states\n",
    "\n",
    "We consider a one-period model with a finite state space $\\Omega=\\{\\omega_1,\\dots,\\omega_N\\}$ and probabilities $p(\\omega)$.\n",
    "Two assets produce (random) payoffs $X_1(\\omega)$ and $X_2(\\omega)$. A single outcome $\\omega_k$ therefore lives as a point in the plane.\n",
    "\n",
    "From these primitives we derive the familiar summary statistics:\n",
    "\n",
    "- expectations $\\mu_i=\\mathbb{E}[X_i]$ (average payoff),\n",
    "- variances $\\sigma_i^2=\\mathbb{V}[X_i]$ (spread of each payoff around its mean),\n",
    "- covariance $\\sigma_{12}=\\mathrm{Cov}(X_1,X_2)$ (do payoffs move together or offset each other?),\n",
    "- correlation $\\rho_{12}=\\sigma_{12}/(\\sigma_1\\sigma_2)$ (a scale-free version of covariance), and\n",
    "- the covariance matrix $\\Sigma$ collecting all pairwise covariances.\n",
    "\n",
    "You can picture $X_1$ and $X_2$ as **two coordinates of the same economic world** (for example, payoffs of two assets in the same scenario set). Covariance will tell us whether that world tilts along the $(1,1)$ direction (strong positive co-movement), the $(1,-1)$ direction (hedging), or something in between.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "240c23ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example economy: 6 discrete states\n",
    "states = np.array([\"Boom\", \"Good\", \"Normal\", \"Soft\", \"Recession\", \"Crisis\"])\n",
    "p = np.array([0.10, 0.20, 0.30, 0.20, 0.15, 0.05])  # probabilities\n",
    "\n",
    "# Two assets: state-contingent payoffs (could be returns or cashflows)\n",
    "X1 = np.array([1.40, 1.20, 1.05, 0.95, 0.80, 0.60])\n",
    "X2 = np.array([1.15, 1.10, 1.02, 0.98, 0.90, 0.75])\n",
    "\n",
    "X = np.column_stack([X1, X2])\n",
    "\n",
    "mu, Sigma = cov_from_states(X, p)\n",
    "rho = corr_from_cov(Sigma)\n",
    "\n",
    "print(\"mu =\", mu)\n",
    "print(\"Sigma =\\n\", Sigma)\n",
    "print(\"rho =\\n\", rho)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b14f067b",
   "metadata": {},
   "source": [
    "### 1.1 Visualize the state cloud in payoff space\n",
    "\n",
    "Each state $\\omega_k$ is a point $(X_1(\\omega_k),X_2(\\omega_k))$ in $\\mathbb{R}^2$.\n",
    "\n",
    "Mentally, think of **throwing one dart per state** at the payoff plane: dense regions of darts show typical joint outcomes, while extreme darts correspond to tail states. The rest of the notebook is about reading structure from this cloud.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f28832c",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(7, 5))\n",
    "ax.scatter(X1, X2)\n",
    "\n",
    "for i, s in enumerate(states):\n",
    "    ax.annotate(s, (X1[i], X2[i]), textcoords=\"offset points\", xytext=(6, 6))\n",
    "\n",
    "ax.set_xlabel(\"Asset 1 payoff $X_1$\")\n",
    "ax.set_ylabel(\"Asset 2 payoff $X_2$\")\n",
    "ax.set_title(\"Discrete states as a cloud in payoff space\")\n",
    "ax.grid(True, alpha=0.3)\n",
    "save_fig(fig, \"fig_cov_state_cloud.png\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6df477c0",
   "metadata": {},
   "source": [
    "### 1.2 Centered payoffs and covariance as a weighted inner product\n",
    "\n",
    "Let $\\mu=\\mathbb{E}[X]$ and $X^c(\\omega)=X(\\omega)-\\mu$. Then:\n",
    "\n",
    "$$\n",
    "\\Sigma=\\mathbb{E}\\left[(X-\\mu)(X-\\mu)^\\top\\right].\n",
    "$$\n",
    "\n",
    "In components:\n",
    "\n",
    "$$\n",
    "\\sigma_{12} = \\sum_{k=1}^N p_k \\,(X_1(\\omega_k)-\\mu_1)(X_2(\\omega_k)-\\mu_2).\n",
    "$$\n",
    "\n",
    "So covariance is a **weighted inner product of centered payoffs**: we multiply deviations of $X_1$ and $X_2$ in each state and average them with state probabilities.\n",
    "\n",
    "- **Sign:** positive if deviations tend to align (large with large, small with small); negative if they offset (one large when the other is small).\n",
    "- **Units:** covariance has units of $X_1\\cdot X_2$, so it depends on scaling; correlation is unitless and lives in $[-1,1]$.\n",
    "\n",
    "A useful picture is to imagine **two centered time series as arrows** in a high-dimensional space (one coordinate per state or date). Covariance then measures the cosine of the angle between these arrows, scaled by their lengths.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c7a1a25",
   "metadata": {},
   "outputs": [],
   "source": [
    "Xc = X - mu\n",
    "\n",
    "sigma12 = Sigma[0, 1]\n",
    "sigma1 = np.sqrt(Sigma[0, 0])\n",
    "sigma2 = np.sqrt(Sigma[1, 1])\n",
    "rho12 = sigma12 / (sigma1 * sigma2)\n",
    "\n",
    "print(\"Centered states Xc (rows=states, cols=assets):\\n\", Xc)\n",
    "print(f\"\\nσ1 = {sigma1:.6f}  σ2 = {sigma2:.6f}\")\n",
    "print(f\"σ12 = {sigma12:.6f}  ρ12 = {rho12:.6f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f57d6d55",
   "metadata": {},
   "source": [
    "## 2) Geometry: covariance ellipse and principal axes\n",
    "\n",
    "For a symmetric covariance matrix, eigenvectors are orthonormal directions $u_1,u_2$ and eigenvalues $\\lambda_1\\ge\\lambda_2\\ge 0$ satisfy:\n",
    "\n",
    "$$\n",
    "\\Sigma u_i = \\lambda_i u_i,\\quad u_i^\\top u_j=\\delta_{ij}.\n",
    "$$\n",
    "\n",
    "Key fact (unit vector $u$):\n",
    "\n",
    "$$\n",
    "\\mathbb{V}[u^\\top X] = u^\\top \\Sigma u.\n",
    "$$\n",
    "\n",
    "If $u$ is an eigenvector, then $u^\\top \\Sigma u = \\lambda$.  So eigenvalues are **variances along eigenvector directions**.\n",
    "\n",
    "To visualize this, we draw the **1-sigma ellipse** of the centered payoff cloud. For a bivariate normal distribution, the 1-sigma ellipse contains roughly $68\\%$ of the probability mass, just like the familiar $\\pm 1$ standard deviation interval on the real line. In our finite-state picture, it behaves like a **rubber band** wrapped around the most typical states, aligned with the directions of greatest and smallest variability.\n",
    "\n",
    "Formally, we map the unit circle through $\\sqrt{\\Sigma}$: every point on the circle is stretched more along high-variance eigenvector directions and less along low-variance ones. The result is an ellipse whose axes line up with eigenvectors and whose axis lengths are proportional to $\\sqrt{\\lambda_i}$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a2bfb35",
   "metadata": {},
   "outputs": [],
   "source": [
    "vals, vecs = eig_sorted(Sigma)\n",
    "\n",
    "# sqrt(Sigma) from eigen-decomposition: Sigma = Q Λ Q^T  => sqrt(Sigma)=Q sqrt(Λ) Q^T\n",
    "sqrtSigma = vecs @ np.diag(np.sqrt(np.maximum(vals, 0))) @ vecs.T\n",
    "\n",
    "theta = np.linspace(0, 2*np.pi, 400)\n",
    "circle = np.column_stack([np.cos(theta), np.sin(theta)])\n",
    "ellipse = circle @ sqrtSigma.T\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(7, 5))\n",
    "ax.scatter(Xc[:, 0], Xc[:, 1], label=\"Centered states\")\n",
    "ax.plot(ellipse[:, 0], ellipse[:, 1], linewidth=2, label=\"1-sigma ellipse\")\n",
    "\n",
    "ax.axhline(0, linewidth=1)\n",
    "ax.axvline(0, linewidth=1)\n",
    "\n",
    "ax.set_xlabel(\"Centered payoff $X_1-\\\\mu_1$\")\n",
    "ax.set_ylabel(\"Centered payoff $X_2-\\\\mu_2$\")\n",
    "ax.set_title(\"Centered state cloud + covariance ellipse\")\n",
    "ax.grid(True, alpha=0.3)\n",
    "ax.legend()\n",
    "save_fig(fig, \"fig_cov_ellipse.png\")\n",
    "plt.show()\n",
    "\n",
    "print(\"Eigenvalues (descending):\", vals)\n",
    "print(\"Eigenvectors (columns):\\n\", vecs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07b28ca9",
   "metadata": {},
   "source": [
    "### 2.1 Draw eigenvectors on the ellipse\n",
    "\n",
    "Eigenvectors are the **principal axes** of the ellipse; $\\sqrt{\\lambda_i}$ are the axis lengths (for the 1-sigma ellipse).\n",
    "\n",
    "You can think of $u_1$ as the direction of **maximum spread** of the payoff cloud: if you project all centered states onto $u_1$, you see the largest variance. The second eigenvector $u_2$ is orthogonal to $u_1$ and captures the remaining variation. In factor-language, $u_1$ is the dominant “factor portfolio” and $u_2$ is a smaller, orthogonal one.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f1a32a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "u1 = vecs[:, 0]\n",
    "u2 = vecs[:, 1]\n",
    "\n",
    "# We care about directions, so give both eigenvector arrows\n",
    "# the same visual length based on the larger eigenvalue.\n",
    "length = 1.1 * np.sqrt(vals[0])\n",
    "scale1 = length * 1.25\n",
    "scale2 = length * 0.5\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(9, 6))\n",
    "\n",
    "ax.plot(ellipse[:, 0], ellipse[:, 1], linewidth=2, label=\"1-sigma ellipse\")\n",
    "ax.scatter(Xc[:, 0], Xc[:, 1], label=\"Centered states\")\n",
    "\n",
    "ax.arrow(\n",
    "    0,\n",
    "    0,\n",
    "    u1[0] * scale1,\n",
    "    u1[1] * scale1,\n",
    "    length_includes_head=True,\n",
    "    head_width=0.008,\n",
    "    head_length=0.02,\n",
    ")\n",
    "ax.arrow(\n",
    "    0,\n",
    "    0,\n",
    "    u2[0] * scale2,\n",
    "    u2[1] * scale2,\n",
    "    length_includes_head=True,\n",
    "    head_width=0.008,\n",
    "    head_length=0.02,\n",
    ")\n",
    "\n",
    "ax.annotate(\n",
    "    \"$u_1$ (PC1)\",\n",
    "    xy=(u1[0] * scale1, u1[1] * scale1),\n",
    "    textcoords=\"offset points\",\n",
    "    xytext=(10, -15),\n",
    "    ha=\"left\",\n",
    "    va=\"top\",\n",
    ")\n",
    "ax.annotate(\n",
    "    \"$u_2$ (PC2)\",\n",
    "    xy=(u2[0] * scale2, u2[1] * scale2),\n",
    "    textcoords=\"offset points\",\n",
    "    xytext=(10, 10),\n",
    "    ha=\"left\",\n",
    "    va=\"bottom\",\n",
    ")\n",
    "\n",
    "ax.axhline(0, linewidth=1)\n",
    "ax.axvline(0, linewidth=1)\n",
    "\n",
    "ax.set_xlabel(\"Centered payoff $X_1-\\mu_1$\")\n",
    "ax.set_ylabel(\"Centered payoff $X_2-\\mu_2$\")\n",
    "ax.set_title(\n",
    "    \"Eigenvectors = principal axes; \"\n",
    "    \"sqrt(eigenvalues) = axis lengths\",\n",
    ")\n",
    "ax.grid(True, alpha=0.3)\n",
    "ax.legend(loc=\"upper left\")\n",
    "ax.set_aspect(\"equal\", adjustable=\"box\")\n",
    "ax.margins(0.15)\n",
    "\n",
    "save_fig(fig, \"fig_cov_eigenvectors.png\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c47b7c0b",
   "metadata": {},
   "source": [
    "## 3) Closed-form eigenvalues/eigenvectors for the 2×2 covariance matrix\n",
    "\n",
    "Write:\n",
    "\n",
    "$$\n",
    "\\Sigma=\\begin{pmatrix} a & c\\\\ c & b\\end{pmatrix},\n",
    "\\quad a=\\sigma_1^2,\\; b=\\sigma_2^2,\\; c=\\sigma_{12}.\n",
    "$$\n",
    "\n",
    "Eigenvalues:\n",
    "\n",
    "$$\n",
    "\\lambda_{\\pm} = \\frac{(a+b)\\pm\\sqrt{(a-b)^2+4c^2}}{2}.\n",
    "$$\n",
    "\n",
    "A corresponding eigenvector can be taken (up to scale) as:\n",
    "\n",
    "$$\n",
    "u(\\lambda)\\propto \\begin{pmatrix} c\\\\ \\lambda-a\\end{pmatrix}\\quad (c\\neq 0).\n",
    "$$\n",
    "\n",
    "Eigenvectors are defined only up to sign: $u$ and $-u$ represent the same direction.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f2de596",
   "metadata": {},
   "outputs": [],
   "source": [
    "a, b, c = Sigma[0, 0], Sigma[1, 1], Sigma[0, 1]\n",
    "disc = np.sqrt((a - b)**2 + 4*c**2)\n",
    "\n",
    "lam_plus  = 0.5*((a + b) + disc)\n",
    "lam_minus = 0.5*((a + b) - disc)\n",
    "\n",
    "print(f\"Analytic eigenvalues: {lam_plus:.6f} {lam_minus:.6f}\")\n",
    "print(\"NumPy eigenvalues    :\", vals)\n",
    "\n",
    "u_plus  = unit(np.array([c, lam_plus  - a])) if abs(c) > 1e-14 else unit(np.array([1.0, 0.0]))\n",
    "u_minus = unit(np.array([c, lam_minus - a])) if abs(c) > 1e-14 else unit(np.array([0.0, 1.0]))\n",
    "\n",
    "err_plus = np.linalg.norm(Sigma @ u_plus  - lam_plus * u_plus)\n",
    "err_minus = np.linalg.norm(Sigma @ u_minus - lam_minus * u_minus)\n",
    "\n",
    "print(\"\\nCheck Σu=λu (norm errors):\")\n",
    "print(f\"  +: {err_plus:.3e}\")\n",
    "print(f\"  -: {err_minus:.3e}\")\n",
    "\n",
    "align_plus = abs(np.dot(u_plus,  vecs[:, 0]))\n",
    "align_minus = abs(np.dot(u_minus, vecs[:, 1]))\n",
    "\n",
    "print(\"\\nDirection alignment with NumPy eigenvectors (abs dot):\")\n",
    "print(f\"  + vs u1: {align_plus:.6f}\")\n",
    "print(f\"  - vs u2: {align_minus:.6f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2cbd0ef",
   "metadata": {},
   "source": [
    "## 4) Positive semidefinite (PSD) vs positive definite (PD)\n",
    "\n",
    "For symmetric matrices:\n",
    "\n",
    "- **PSD:** $z^\\top\\Sigma z \\ge 0$ for all $z$.\n",
    "- **PD:**  $z^\\top\\Sigma z > 0$ for all nonzero $z$.\n",
    "\n",
    "Here $z^\\top\\Sigma z$ is the variance of the scalar random variable $z^\\top X$. Covariance matrices are PSD because $z^\\top\\Sigma z=\\mathbb{V}[z^\\top X]\\ge 0$ by definition.\n",
    "\n",
    "- **PSD intuition:** every portfolio variance is nonnegative, but some directions $z$ may have zero variance (perfect linear dependence).\n",
    "- **PD intuition:** every nontrivial direction carries strictly positive risk; there are no exact linear redundancies.\n",
    "\n",
    "### 2×2 PSD condition\n",
    "For $\\Sigma=\\begin{pmatrix}a&c\\\\c&b\\end{pmatrix}$:\n",
    "\n",
    "$$\n",
    "\\Sigma\\succeq 0 \\iff a\\ge 0,\\; b\\ge 0,\\; ab-c^2\\ge 0.\n",
    "$$\n",
    "\n",
    "Equivalently, all eigenvalues are nonnegative.\n",
    "\n",
    "The determinant\n",
    "$$\n",
    "\\det(\\Sigma)=ab-c^2\n",
    "$$\n",
    "measures how “2D” the risk is: $\\det=0$ implies singular (rank 1) covariance, so all uncertainty lives on a single line in payoff space.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51ec812d",
   "metadata": {},
   "outputs": [],
   "source": [
    "det = np.linalg.det(Sigma)\n",
    "print(f\"det(Sigma) = {det:.6f}\")\n",
    "print(\"Eigenvalues =\", vals)\n",
    "print(\"PSD (all eigenvalues >= 0) =\", np.all(vals >= -1e-12))\n",
    "print(\"PD  (all eigenvalues  > 0) =\", np.all(vals >  1e-12))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77bb370e",
   "metadata": {},
   "source": [
    "### 4.1 Visual PSD test: sample random directions\n",
    "\n",
    "Compute $q(z)=z^\\top\\Sigma z$ for many unit vectors $z$.  \n",
    "For PSD matrices, $q(z)$ never goes negative.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a1fce2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = np.random.default_rng(42)\n",
    "\n",
    "Z = rng.normal(size=(20000, 2))\n",
    "Z = Z / np.linalg.norm(Z, axis=1, keepdims=True)\n",
    "q = np.einsum(\"ni,ij,nj->n\", Z, Sigma, Z)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(7, 4))\n",
    "ax.hist(q, bins=60)\n",
    "ax.set_title(\"PSD check: histogram of $z^T\\Sigma z$ over random unit vectors $z$\")\n",
    "ax.set_xlabel(\"$z^T\\Sigma z$\")\n",
    "ax.set_ylabel(\"count\")\n",
    "ax.grid(True, alpha=0.3)\n",
    "save_fig(fig, \"fig_cov_psd_hist.png\")\n",
    "plt.show()\n",
    "\n",
    "print(f\"min q = {q.min():.6f}  max q = {q.max():.6f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b63616ed",
   "metadata": {},
   "source": [
    "### 4.2 A non-PSD “covariance matrix” (what breaks)\n",
    "\n",
    "If you inflate the off-diagonal $c$ beyond the PSD bound $|c|\\le \\sqrt{ab}$, one eigenvalue becomes negative.\n",
    "Then some directions $z$ have $z^\\top\\Sigma z<0$, which is impossible for a true covariance matrix.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f9c644c",
   "metadata": {},
   "outputs": [],
   "source": [
    "c_bad = 1.5*np.sqrt(a*b)  # violates |c| <= sqrt(ab)\n",
    "Sigma_bad = np.array([[a, c_bad], [c_bad, b]])\n",
    "\n",
    "vals_bad, vecs_bad = eig_sorted(Sigma_bad)\n",
    "print(\"Sigma_bad =\\n\", Sigma_bad)\n",
    "det_bad = np.linalg.det(Sigma_bad)\n",
    "print(f\"det(Sigma_bad) = {det_bad:.6f}\")\n",
    "print(\"eigenvalues =\", vals_bad)\n",
    "\n",
    "Z = rng.normal(size=(20000, 2))\n",
    "Z = Z / np.linalg.norm(Z, axis=1, keepdims=True)\n",
    "q_bad = np.einsum(\"ni,ij,nj->n\", Z, Sigma_bad, Z)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(7, 4))\n",
    "ax.hist(q_bad, bins=60)\n",
    "ax.set_title(\"Non-PSD example: $z^T\\Sigma z$ can be negative\")\n",
    "ax.set_xlabel(\"$z^T\\Sigma z$\")\n",
    "ax.set_ylabel(\"count\")\n",
    "ax.grid(True, alpha=0.3)\n",
    "save_fig(fig, \"fig_cov_nonpsd_hist.png\")\n",
    "plt.show()\n",
    "\n",
    "share_neg = (q_bad < 0).mean()\n",
    "print(f\"min q_bad = {q_bad.min():.6f}  share negative = {share_neg:.6f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3f49832",
   "metadata": {},
   "source": [
    "## 5) Portfolio variance in the two-asset case\n",
    "\n",
    "Let weights $w=(w_1,w_2)$ and portfolio payoff $X_p=w_1X_1+w_2X_2$. Then:\n",
    "\n",
    "$$\n",
    "\\mathbb{V}[X_p]=w^\\top\\Sigma w.\n",
    "$$\n",
    "\n",
    "This formula is just the geometric story in portfolio language: we pick a direction $w$ in payoff space, and $w^\\top\\Sigma w$ gives the variance of the projection of $X$ onto that direction.\n",
    "\n",
    "With budget constraint $w_1+w_2=1$ (so $w_2=1-w_1$), this becomes a quadratic function of $w_1$ that you can visualize as a parabola over the weight axis.\n",
    "\n",
    "### Minimum variance weight (only $w_1+w_2=1$)\n",
    "$$\n",
    "w_1^\\star=\\frac{\\sigma_2^2-\\sigma_{12}}{\\sigma_1^2+\\sigma_2^2-2\\sigma_{12}},\\quad w_2^\\star=1-w_1^\\star.\n",
    "$$\n",
    "\n",
    "At this point, the directional variance $w^\\top\\Sigma w$ is minimized; in the geometric picture this corresponds to **tilting the weight vector** so that the 1-sigma ellipse is as “narrow” as possible along the portfolio direction.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e1d6518",
   "metadata": {},
   "outputs": [],
   "source": [
    "w1_star = (b - c) / (a + b - 2*c)\n",
    "w2_star = 1 - w1_star\n",
    "var_star = w1_star**2*a + w2_star**2*b + 2*w1_star*w2_star*c\n",
    "\n",
    "w1_min = min(-0.5, w1_star - 0.2)\n",
    "w1_max = max(1.5, w1_star + 0.2)\n",
    "w1 = np.linspace(w1_min, w1_max, 501)\n",
    "w2 = 1 - w1\n",
    "\n",
    "var_p = (w1**2)*a + (w2**2)*b + 2*w1*w2*c\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(7, 5))\n",
    "ax.plot(w1, var_p, label=\"portfolio variance\")\n",
    "ax.vlines(\n",
    "    w1_star,\n",
    "    0.0,\n",
    "    var_star,\n",
    "    colors=\"tab:red\",\n",
    "    linewidth=1.5,\n",
    "    linestyles=\"--\",\n",
    ")\n",
    "ax.scatter(\n",
    "    [w1_star],\n",
    "    [var_star],\n",
    "    color=\"tab:red\",\n",
    "    zorder=3,\n",
    "    label=\"min-variance portfolio $w_1^*$\",\n",
    ")\n",
    "\n",
    "ax.set_title(\"Portfolio variance $w^T\\Sigma w$ vs weight $w_1$ with $w_2=1-w_1$\")\n",
    "ax.set_xlabel(\"$w_1$\")\n",
    "ax.set_ylabel(\"Variance\")\n",
    "ax.grid(True, alpha=0.3)\n",
    "ax.legend()\n",
    "save_fig(fig, \"fig_cov_portfolio_variance.png\")\n",
    "plt.show()\n",
    "\n",
    "print(f\"w* = ({w1_star:.6f}, {w2_star:.6f})\")\n",
    "print(f\"var(w*) = {var_star:.6f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3b26f8c",
   "metadata": {},
   "source": [
    "### 5.1 Eigenvalues via the Rayleigh quotient (risk extremes)\n",
    "\n",
    "If instead you constrain $\\|w\\|_2=1$ (unit leverage), then:\n",
    "\n",
    "$$\n",
    "\\lambda_{\\min}\\le \\frac{w^\\top\\Sigma w}{w^\\top w}\\le \\lambda_{\\max}.\n",
    "$$\n",
    "\n",
    "The maximizing $w$ is the top eigenvector $u_1$; the minimizing $w$ is $u_2$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b2d4bd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "W = rng.normal(size=(30000, 2))\n",
    "rq = np.einsum(\"ni,ij,nj->n\", W, Sigma, W) / np.einsum(\"ni,ni->n\", W, W)\n",
    "\n",
    "rq_min = rq.min()\n",
    "rq_max = rq.max()\n",
    "lam_min = vals.min()\n",
    "lam_max = vals.max()\n",
    "\n",
    "print(\n",
    "    \"Rayleigh quotient min/max (empirical): \"\n",
    "    f\"{rq_min:.6f} {rq_max:.6f}\",\n",
    ")\n",
    "print(\n",
    "    \"Eigenvalue min/max: \"\n",
    "    f\"{lam_min:.6f} {lam_max:.6f}\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c4d3c27",
   "metadata": {},
   "source": [
    "## 6) Diagonalization: rotate into the eigenbasis (principal components)\n",
    "\n",
    "Because $\\Sigma$ is symmetric:\n",
    "\n",
    "$$\n",
    "\\Sigma = Q\\Lambda Q^\\top\n",
    "$$\n",
    "\n",
    "with orthonormal $Q=[u_1\\;u_2]$ and diagonal $\\Lambda=\\mathrm{diag}(\\lambda_1,\\lambda_2)$.\n",
    "\n",
    "If you rotate centered payoffs $Y = X_c Q$, then:\n",
    "\n",
    "$$\n",
    "\\mathrm{Cov}(Y)=\\Lambda.\n",
    "$$\n",
    "\n",
    "So in the eigenbasis, the covariance matrix becomes diagonal: **uncorrelated principal components**. Each component is a factor portfolio with variance equal to one eigenvalue, and the 1-sigma ellipse becomes an axis-aligned ellipse (in fact, a circle after rescaling).\n",
    "\n",
    "You can think of this rotation as **finding a coordinate system where the payoff cloud is as simple as possible**: all co-movement is pushed into separate, orthogonal axes. This is exactly the viewpoint of principal component analysis (PCA).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcdbb2ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q = vecs\n",
    "Lam = np.diag(vals)\n",
    "\n",
    "err = np.linalg.norm(Sigma - Q @ Lam @ Q.T)\n",
    "print(f\"||Sigma - QΛQ^T|| = {err:.3e}\")\n",
    "print(\"Q^T Sigma Q =\\n\", Q.T @ Sigma @ Q)\n",
    "\n",
    "Y = Xc @ Q\n",
    "_, Sigma_pc = cov_from_states(Y, p)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(7, 5))\n",
    "ax.scatter(Y[:, 0], Y[:, 1])\n",
    "\n",
    "for i, s in enumerate(states):\n",
    "    ax.annotate(s, (Y[i, 0], Y[i, 1]), textcoords=\"offset points\", xytext=(6, 6))\n",
    "\n",
    "ax.axhline(0, linewidth=1)\n",
    "ax.axvline(0, linewidth=1)\n",
    "ax.set_xlabel(\"PC1 coordinate\")\n",
    "ax.set_ylabel(\"PC2 coordinate\")\n",
    "ax.set_title(\"Centered states expressed in the eigenbasis (principal components)\")\n",
    "ax.grid(True, alpha=0.3)\n",
    "save_fig(fig, \"fig_cov_pca_scatter.png\")\n",
    "plt.show()\n",
    "\n",
    "print(\"Covariance in PC coordinates (prob-weighted) =\\n\", Sigma_pc)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba57a79a",
   "metadata": {},
   "source": [
    "## 7) Correlation matrix and scale effects\n",
    "\n",
    "Covariance depends on units. Correlation is scale-free:\n",
    "\n",
    "$$\n",
    "\\rho_{12}=\\frac{\\sigma_{12}}{\\sigma_1\\sigma_2}.\n",
    "$$\n",
    "\n",
    "A standardized covariance matrix is the correlation matrix:\n",
    "\n",
    "$$\n",
    "R = D^{-1/2}\\Sigma D^{-1/2},\\quad D=\\mathrm{diag}(\\sigma_1^2,\\sigma_2^2).\n",
    "$$\n",
    "\n",
    "Eigenvalues/eigenvectors of $R$ describe co-movement structure independent of scale.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b3c2f2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "D_inv_sqrt = np.diag(1 / np.sqrt(np.diag(Sigma)))\n",
    "R = D_inv_sqrt @ Sigma @ D_inv_sqrt\n",
    "\n",
    "vals_R, vecs_R = eig_sorted(R)\n",
    "\n",
    "print(\"R =\\n\", R)\n",
    "print(\"eig(R) =\", vals_R)\n",
    "print(\"eigvecs(R) (cols) =\\n\", vecs_R)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cf3de98",
   "metadata": {},
   "source": [
    "## 8) Estimation: sample covariance convergence (simulation)\n",
    "\n",
    "In practice, $\\Sigma$ is unknown and must be estimated from data (for example, a time series of returns). With small samples it is noisy and may be nearly singular.\n",
    "\n",
    "In this section we simulate a bivariate normal with target covariance $\\Sigma$ and track how the sample covariance converges as we increase the number of observations. You can read the plots as **slowly sharpening versions of the same 1-sigma ellipse**: early on, the sampled cloud is jagged and the estimated ellipse wiggles; later, both settle near the true shape.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a7d9b15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build A such that Sigma = A A^T using eigen-decomposition\n",
    "vals, vecs = eig_sorted(Sigma)\n",
    "A = vecs @ np.diag(np.sqrt(np.maximum(vals, 0)))\n",
    "\n",
    "rng = np.random.default_rng(0)\n",
    "\n",
    "def sample_cov(n):\n",
    "    Z = rng.normal(size=(n, 2))\n",
    "    Xsim = Z @ A.T  # mean 0, cov approx Sigma\n",
    "    S = np.cov(Xsim.T, bias=False)  # sample covariance (n-1 denom)\n",
    "    return S\n",
    "\n",
    "ns = np.array([10, 20, 50, 100, 200, 500, 1000, 3000, 10000])\n",
    "errs = []\n",
    "for n in ns:\n",
    "    S = sample_cov(int(n))\n",
    "    errs.append(np.linalg.norm(S - Sigma))\n",
    "\n",
    "errs = np.array(errs)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(7, 4))\n",
    "ax.plot(ns, errs, marker=\"o\")\n",
    "ax.set_xscale(\"log\")\n",
    "ax.set_title(\"Convergence of sample covariance to target Σ (log x-axis)\")\n",
    "ax.set_xlabel(\"sample size n\")\n",
    "ax.set_ylabel(\"||S_n - Σ||\")\n",
    "ax.grid(True, alpha=0.3)\n",
    "save_fig(fig, \"fig_cov_sample_convergence.png\")\n",
    "plt.show()\n",
    "\n",
    "print(\"errors:\", errs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb7c077f",
   "metadata": {},
   "source": [
    "## 9) Near-singularity and rank deficiency\n",
    "\n",
    "A 2×2 covariance matrix becomes singular exactly when:\n",
    "\n",
    "$$\n",
    "\\det(\\Sigma)=ab-c^2=0\\quad\\Longleftrightarrow\\quad\\lvert\\rho_{12}\\rvert=1.\n",
    "$$\n",
    "\n",
    "Then one eigenvalue is 0: the uncertainty is effectively one-dimensional (a “one-factor” structure). Geometrically, the 1-sigma ellipse collapses to a line segment; economically, one payoff is an exact linear combination of the other.\n",
    "\n",
    "This is the limiting case of highly correlated assets in practice: as $\\lvert\\rho_{12}\\rvert$ approaches 1, the ellipse becomes more and more elongated and numerical algorithms start to struggle, because many directions $w$ have almost the same variance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f37a9d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "sigma1 = 0.20\n",
    "sigma2 = 0.15\n",
    "rho_target = 0.999\n",
    "c_near = rho_target * sigma1 * sigma2\n",
    "\n",
    "Sigma_near = np.array([[sigma1**2, c_near], [c_near, sigma2**2]])\n",
    "vals_near, vecs_near = eig_sorted(Sigma_near)\n",
    "\n",
    "print(\"Sigma_near =\\n\", Sigma_near)\n",
    "det_near = np.linalg.det(Sigma_near)\n",
    "print(f\"det(Sigma_near) = {det_near:.6f}\")\n",
    "print(\"eigenvalues =\", vals_near)\n",
    "\n",
    "# visualize ellipse (very elongated)\n",
    "theta = np.linspace(0, 2*np.pi, 400)\n",
    "circle = np.column_stack([np.cos(theta), np.sin(theta)])\n",
    "sqrtSigma_near = vecs_near @ np.diag(np.sqrt(np.maximum(vals_near, 0))) @ vecs_near.T\n",
    "ellipse_near = circle @ sqrtSigma_near.T\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(7, 5))\n",
    "ax.plot(ellipse_near[:, 0], ellipse_near[:, 1], linewidth=2)\n",
    "ax.axhline(0, linewidth=1)\n",
    "ax.axvline(0, linewidth=1)\n",
    "ax.set_title(\"Near-singular covariance: ellipse becomes extremely elongated\")\n",
    "ax.set_xlabel(\"x\")\n",
    "ax.set_ylabel(\"y\")\n",
    "ax.grid(True, alpha=0.3)\n",
    "save_fig(fig, \"fig_cov_near_singular.png\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b79c4f40",
   "metadata": {},
   "source": [
    "## 10) State prices, SDF, and covariance\n",
    "\n",
    "In a one-period world, no-arbitrage pricing can be written in terms of a **stochastic discount factor (SDF)** $m(\\omega)$ or, equivalently, **state prices** $q(\\omega)$. For any asset with payoff $X(\\omega)$ and current price $P$, the pricing equation is\n",
    "\n",
    "$$\n",
    "P = \\mathbb{E}[m X] = \\sum_{\\omega} p(\\omega)\\, m(\\omega) X(\\omega).\n",
    "$$\n",
    "\n",
    "If we work with gross returns $R(\\omega)=X(\\omega)/P$ and take $P=1$, this becomes\n",
    "\n",
    "$$\n",
    "1 = \\mathbb{E}[m R].\n",
    "$$\n",
    "\n",
    "Intuitively, $m(\\omega)$ is high in **bad states** (where marginal utility of wealth is high) and low in **good states**.\n",
    "\n",
    "In this section we construct a simple SDF for our two-asset world, chosen to be larger when the equal-weight ``market\" portfolio does poorly, and visualize how its **covariance with each asset** relates to expected excess returns.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e289c57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple SDF construction tied to the equal-weight market portfolio\n",
    "Rf = 1.02  # risk-free gross return (2% per period)\n",
    "\n",
    "w_market = np.array([0.5, 0.5])\n",
    "R_market = X @ w_market\n",
    "mu_market = (p * R_market).sum()\n",
    "\n",
    "# Stylized SDF: high when the market does badly, low when it does well\n",
    "m_raw = 1.0 / R_market\n",
    "k = 1.0 / (Rf * (p * m_raw).sum())\n",
    "m = k * m_raw\n",
    "\n",
    "# Check pricing of risk-free and risky assets via E[m R] ≈ 1\n",
    "price_rf = (p * m * Rf).sum()\n",
    "price_1 = (p * m * X1).sum()\n",
    "price_2 = (p * m * X2).sum()\n",
    "\n",
    "print(f\"Price of risk-free (target 1.000000): {price_rf:.6f}\")\n",
    "print(f\"Price of asset 1    (target 1.000000): {price_1:.6f}\")\n",
    "print(f\"Price of asset 2    (target 1.000000): {price_2:.6f}\")\n",
    "\n",
    "# Covariance of SDF with each asset return\n",
    "m_bar = (p * m).sum()\n",
    "cov_m_R1 = (p * (m - m_bar) * (X1 - mu[0])).sum()\n",
    "cov_m_R2 = (p * (m - m_bar) * (X2 - mu[1])).sum()\n",
    "\n",
    "print(f\"Cov(m, R1) = {cov_m_R1:.6f}\")\n",
    "print(f\"Cov(m, R2) = {cov_m_R2:.6f}\")\n",
    "\n",
    "# Visualize payoffs and SDF across states (sorted by market return)\n",
    "idx = np.argsort(R_market)\n",
    "states_sorted = states[idx]\n",
    "R_market_sorted = R_market[idx]\n",
    "m_sorted = m[idx]\n",
    "\n",
    "x = np.arange(len(states_sorted))\n",
    "\n",
    "fig, ax1 = plt.subplots(figsize=(8, 4))\n",
    "# Bars centered at integer positions for the market payoff\n",
    "bars = ax1.bar(x, R_market_sorted, width=0.6, label=\"Market payoff R_m\")\n",
    "ax1.set_ylabel(\"Gross return\")\n",
    "ax1.set_xticks(x)\n",
    "ax1.set_xticklabels(states_sorted, rotation=30, ha=\"right\")\n",
    "y_min = 0.0\n",
    "y_max = R_market_sorted.max() * 1.25\n",
    "ax1.set_ylim(y_min, y_max)\n",
    "\n",
    "ax2 = ax1.twinx()\n",
    "ax2.plot(\n",
    "    x,\n",
    "    m_sorted,\n",
    "    color=\"tab:orange\",\n",
    "    marker=\"o\",\n",
    "    label=\"SDF m(ω)\",\n",
    ")\n",
    "ax2.set_ylabel(\"SDF level\")\n",
    "m_min = m_sorted.min() * 0.95\n",
    "m_max = m_sorted.max() * 1.05\n",
    "ax2.set_ylim(m_min, m_max)\n",
    "ax2.grid(False)\n",
    "\n",
    "lines1, labels1 = ax1.get_legend_handles_labels()\n",
    "lines2, labels2 = ax2.get_legend_handles_labels()\n",
    "ax1.legend(\n",
    "    lines1 + lines2,\n",
    "    labels1 + labels2,\n",
    "    loc=\"upper center\",\n",
    ")\n",
    "\n",
    "ax1.set_title(\"Stylized SDF: high in bad states, low in good states\")\n",
    "fig.tight_layout()\n",
    "save_fig(fig, \"fig_cov_sdf_states.png\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "938d933e",
   "metadata": {},
   "source": [
    "## 11) Mean–variance pricing and a CAPM-style view\n",
    "\n",
    "In mean–variance theory we approximate investors by their preferences over expected return and return variance. With a risk-free gross return $R_f$ and a risky market portfolio $R_m$, the Capital Asset Pricing Model (CAPM) states\n",
    "\n",
    "$$\n",
    "\\mathbb{E}[R_i] - R_f = \\beta_i \\bigl(\\mathbb{E}[R_m] - R_f\\bigr),\\qquad \\beta_i = \\frac{\\mathrm{Cov}(R_i,R_m)}{\\mathrm{Var}(R_m)}.\n",
    "$$\n",
    "\n",
    "We now treat an equal-weight portfolio of our two assets as the market, generate many random portfolios, and visualize both the **mean–variance cloud** and the **Security Market Line** (expected excess return versus beta).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ee65560-9d26-42fd-8ca4-fc3ae199db1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Equal-weight market portfolio and its moments\n",
    "Rf = 1.02\n",
    "w_market = np.array([0.5, 0.5])\n",
    "R_market = X @ w_market\n",
    "mu_market = (p * R_market).sum()\n",
    "var_market = (p * (R_market - mu_market)**2).sum()\n",
    "\n",
    "# Random portfolios on the two-asset span\n",
    "rng = np.random.default_rng(123)\n",
    "W = rng.normal(size=(300, 2))\n",
    "W = W / W.sum(axis=1, keepdims=True)\n",
    "\n",
    "mu_port = W @ mu\n",
    "var_port = np.einsum(\"ni,ij,nj->n\", W, Sigma, W)\n",
    "sigma_port = np.sqrt(var_port)\n",
    "\n",
    "# Mean–variance picture (σ on x-axis, E[R] on y-axis)\n",
    "fig, ax = plt.subplots(figsize=(7, 5))\n",
    "asset_sigma = np.sqrt(np.diag(Sigma))\n",
    "ax.scatter(sigma_port, mu_port, alpha=0.4, s=15, label=\"random portfolios\")\n",
    "ax.scatter(asset_sigma, mu, color='yellow', label=\"individual assets\")\n",
    "ax.scatter(\n",
    "    np.sqrt(var_market),\n",
    "    mu_market,\n",
    "    color=\"tab:red\",\n",
    "    label=\"market (50/50)\",\n",
    ")\n",
    "\n",
    "ax.set_xlabel(\"Standard deviation\")\n",
    "ax.set_ylabel(\"Expected gross return\")\n",
    "ax.set_title(\"Mean–variance picture for the two-asset world\")\n",
    "ax.grid(True, alpha=0.3)\n",
    "ax.legend()\n",
    "save_fig(fig, \"fig_cov_mean_variance.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f20b1af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Security Market Line: expected excess return vs beta\n",
    "beta_assets = (Sigma @ w_market) / var_market\n",
    "beta1, beta2 = beta_assets\n",
    "\n",
    "beta_port = (W @ (Sigma @ w_market)) / var_market\n",
    "excess_mu_assets = mu - Rf\n",
    "excess_mu_port = mu_port - Rf\n",
    "excess_mu_market = mu_market - Rf\n",
    "\n",
    "beta_grid = np.linspace(beta_port.min() - 0.1, beta_port.max() + 0.1, 50)\n",
    "excess_sml = beta_grid * (mu_market - Rf)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(7, 5))\n",
    "ax.scatter(beta_port, excess_mu_port, alpha=0.4, s=15, label=\"random portfolios\")\n",
    "ax.scatter(beta_assets, excess_mu_assets, color=\"yellow\", label=\"assets\")\n",
    "ax.scatter(\n",
    "    [1.0],\n",
    "    [excess_mu_market],\n",
    "    color=\"tab:red\",\n",
    "    label=\"market (beta=1)\",\n",
    ")\n",
    "ax.plot(beta_grid, excess_sml, color=\"blue\", alpha=0.8, linewidth=2, label=\"Security Market Line\")\n",
    "\n",
    "ax.set_xlabel(\"Beta relative to market\")\n",
    "ax.set_ylabel(\"Expected excess return E[R] - R_f\")\n",
    "ax.set_title(\"CAPM-style Security Market Line from two-asset world\")\n",
    "ax.grid(True, alpha=0.3)\n",
    "ax.legend()\n",
    "save_fig(fig, \"fig_cov_sml.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83cdbb62",
   "metadata": {},
   "source": [
    "## 12) A two-asset factor model\n",
    "\n",
    "Finally, we can view the equal-weight portfolio $R_m$ as a **single factor** and express each asset return as\n",
    "\n",
    "$$\n",
    "R_i = \\alpha_i + \\beta_i R_m + \\varepsilon_i,\\qquad \\mathbb{E}[\\varepsilon_i]=0,\\ \\mathrm{Cov}(\\varepsilon_i,R_m)=0.\n",
    "$$\n",
    "\n",
    "This is the simplest form of a one-factor model. It shows how covariance with the factor (through $\\beta_i$) drives expected returns while idiosyncratic noise $\\varepsilon_i$ averages out in portfolios.\n",
    "\n",
    "We estimate $\\alpha_i$ and $\\beta_i$ in the discrete state world and visualize the regression lines.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "928e7fdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "Rf = 1.02\n",
    "w_market = np.array([0.5, 0.5])\n",
    "R_market = X @ w_market\n",
    "mu_market = (p * R_market).sum()\n",
    "var_market = (p * (R_market - mu_market)**2).sum()\n",
    "\n",
    "def factor_reg(R, name):\n",
    "    \"\"\"Weighted one-factor regression on the market portfolio.\"\"\"\n",
    "    mu_R = (p * R).sum()\n",
    "    cov_RM = (p * (R - mu_R) * (R_market - mu_market)).sum()\n",
    "    beta = cov_RM / var_market\n",
    "    alpha = mu_R - beta * mu_market\n",
    "    eps = R - (alpha + beta * R_market)\n",
    "    mu_eps = (p * eps).sum()\n",
    "    var_eps = (p * (eps - mu_eps)**2).sum()\n",
    "    print(\n",
    "        f\"{name}: alpha = {alpha:.6f}, beta = {beta:.6f}, \"\n",
    "        f\"Var(eps) = {var_eps:.6f}\",\n",
    "    )\n",
    "    return alpha, beta, eps\n",
    "\n",
    "alpha1, beta1, eps1 = factor_reg(X1, \"Asset 1\")\n",
    "alpha2, beta2, eps2 = factor_reg(X2, \"Asset 2\")\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(11, 4), sharex=True, sharey=True)\n",
    "for ax, R, alpha, beta, name in [\n",
    "    (axes[0], X1, alpha1, beta1, \"Asset 1\"),\n",
    "    (axes[1], X2, alpha2, beta2, \"Asset 2\"),\n",
    "]:\n",
    "    ax.scatter(R_market, R)\n",
    "    ax.set_title(name)\n",
    "    ax.set_xlabel(\"Market return R_m\")\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "    Rm_line = np.linspace(R_market.min(), R_market.max(), 50)\n",
    "    R_line = alpha + beta * Rm_line\n",
    "    ax.plot(Rm_line, R_line, color=\"tab:orange\", linewidth=2)\n",
    "\n",
    "axes[0].set_ylabel(\"Asset return R_i\")\n",
    "plt.suptitle(\"One-factor view: assets as alpha + beta × market + noise\")\n",
    "plt.tight_layout()\n",
    "save_fig(fig, \"fig_cov_factor_regression.png\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1aa89a5",
   "metadata": {},
   "source": [
    "## 13) Quick reference (2 assets)\n",
    "\n",
    "Given\n",
    "\n",
    "$$\n",
    "\\Sigma=\\begin{pmatrix}\\sigma_1^2 & \\sigma_{12}\\\\ \\sigma_{12} & \\sigma_2^2\\end{pmatrix},\n",
    "$$\n",
    "\n",
    "- PSD iff $\\sigma_1^2\\ge 0$, $\\sigma_2^2\\ge 0$, $\\sigma_{12}^2\\le \\sigma_1^2\\sigma_2^2$.\n",
    "- Eigenvalues:\n",
    "  $$\n",
    "  \\lambda_{\\pm}=\\frac{(\\sigma_1^2+\\sigma_2^2)\\pm\\sqrt{(\\sigma_1^2-\\sigma_2^2)^2+4\\sigma_{12}^2}}{2}.\n",
    "  $$\n",
    "- Eigenvectors = principal risk directions; eigenvalues = variances along them.\n",
    "- Portfolio variance: $w^\\top\\Sigma w$.\n",
    "- $\\det(\\Sigma)=\\sigma_1^2\\sigma_2^2-\\sigma_{12}^2$ indicates singularity and effective rank.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9ace749",
   "metadata": {},
   "source": [
    "<img src=\"https://hilpisch.com/tpq_logo_bic.png\" width=\"20%\" align=\"right\">\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
